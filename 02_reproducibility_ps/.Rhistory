X2 = c(4, 2, 4, 4, 1, 3, 1),
Y = factor(c("Red", "Red", "Red", "Red", "Blue", "Blue", "Blue"))
)
colors <- ifelse(df$Y == "Red", "red", "blue")
plot(df$X1, df$X2, col = colors, pch = 19,
xlab = "X1", ylab = "X2", main = "SVM Classification")
legend("topright", legend = c("Red", "Blue"), col = c("red", "blue"), pch = 19)
# b.
svmfit <- svm(Y ~ X1 + X2, data = df, kernel = "linear", cost = 10, scale = FALSE)
plot(svmfit, df)
# a.
df <- data.frame(
X1 = c(3, 2, 4, 1, 2, 4, 4),
X2 = c(4, 2, 4, 4, 1, 3, 1),
Y = factor(c("Red", "Red", "Red", "Red", "Blue", "Blue", "Blue"))
)
colors <- ifelse(df$Y == "Red", "red", "blue")
plot(df$X1, df$X2, col = colors, pch = 19,
xlab = "X1", ylab = "X2", main = "Scatter Plot")
legend("topright", legend = c("Red", "Blue"), col = c("red", "blue"), pch = 19)
# b.
svmfit <- svm(Y ~ X1 + X2, data = df, kernel = "linear", cost = 10, scale = FALSE)
plot(svmfit, df)
rm(list = ls(all=TRUE))
P<-c("readr","tidyr","ggplot2", "e1071", "LiblineaR", "ISLR2")
for (i in 1:length(P)) {
ifelse(!require(P[i],character.only=TRUE),install.packages(P[i]),
print(":)"))
library(P[i],character.only=TRUE)
}
rm(P,i)
# a.
df <- data.frame(
X1 = c(3, 2, 4, 1, 2, 4, 4),
X2 = c(4, 2, 4, 4, 1, 3, 1),
Y = factor(c("Red", "Red", "Red", "Red", "Blue", "Blue", "Blue"))
)
colors <- ifelse(df$Y == "Red", "red", "blue")
plot(df$X1, df$X2, col = colors, pch = 19,
xlab = "X1", ylab = "X2", main = "Scatter Plot")
legend("topright", legend = c("Red", "Blue"), col = c("red", "blue"), pch = 19)
# b.
svmfit <- svm(Y ~ X1 + X2, data = df, kernel = "linear", cost = 10, scale = FALSE)
plot(svmfit, df)
df <- data.frame(
X1 = c(3, 2, 4, 1, 2, 4, 4),
X2 = c(4, 2, 4, 4, 1, 3, 1),
Y = factor(c("Red", "Red", "Red", "Red", "Blue", "Blue", "Blue"))
)
colors <- ifelse(df$Y == "Red", "red", "blue")
plot(df$X1, df$X2, col = colors, pch = 19,
xlab = "X1", ylab = "X2", main = "Scatter Plot")
legend("topright", legend = c("Red", "Blue"), col = c("red", "blue"), pch = 19)
svmfit <- svm(Y ~ X1 + X2, data = df, kernel = "linear", cost = 10, scale = FALSE)
plot(svmfit, df,
symbolPalette = c("red", "blue"),
main = "SVM Classification")
svmfit <- svm(Y ~ X1 + X2, data = df, kernel = "linear", cost = 10, scale = FALSE)
plot(svmfit, df,
symbolPalette = c("red", "blue"))
w <- t(svmfit$coefs) %*% svmfit$SV
b <- -svmfit$rho
cat("β0 (intercept):", round(b, 3), "\n")
cat("β1 (X1 coefficient):", round(w[1], 3), "\n")
cat("β2 (X2 coefficient):", round(w[2], 3), "\n")
slope <- -w[1] / w[2]
intercept <- -b / w[2]        # decision boundary
margin1 <- (1 - b) / w[2]     # upper margin
margin2 <- (-1 - b) / w[2]    # lower margin
plot(dat[, c("X1", "X2")], col = dat$Y, pch = 19, xlim = c(0, 5), ylim = c(0, 5))
slope <- -w[1] / w[2]
intercept <- -b / w[2]        # decision boundary
margin1 <- (1 - b) / w[2]     # upper margin
margin2 <- (-1 - b) / w[2]    # lower margin
plot(dat[, c("X1", "X2")], col = df$Y, pch = 19, xlim = c(0, 5), ylim = c(0, 5))
plot(df[, c("X1", "X2")], col = df$Y, pch = 19, xlim = c(0, 5), ylim = c(0, 5))
points(svmfit$SV, pch = 5, cex = 2)
abline(a = intercept, b = slope, col = "black", lwd = 2)
abline(a = margin1, b = slope, col = "blue", lty = 2)
abline(a = margin2, b = slope, col = "blue", lty = 2)
points(svmfit$SV, pch = 8, cex = 2, lwd = 2)
points(svmfit$SV, pch = 8, cex = 2, lwd = 2)
svmfit$index
points(svmfit$SV, pch = 8, cex = 2, lwd = 2)
# f.
svmfit$index #indices of support vectors
# slightly change the 7th obs
dat_moved <- df
dat_moved[7, "X1"] <- dat_moved[7, "X1"] + 0.1
dat_moved[7, "X2"] <- dat_moved[7, "X2"] + 0.1
# refit the SVM
svmfit_moved <- svm(Y ~ ., data = dat_moved, kernel = "linear", cost = 10, scale = FALSE)
# compare hyperplane coefficients
w_original <- t(svmfit$coefs) %*% svmfit$SV
b_original <- -svmfit$rho
w_moved <- t(svmfit_moved$coefs) %*% svmfit_moved$SV
b_moved <- -svmfit_moved$rho
cat("Original w:", round(w_original, 3), ", b:", round(b_original, 3), "\n")
cat("Moved w:  ", round(w_moved, 3), ", b:", round(b_moved, 3), "\n")
svmfit <- svm(Y ~ ., data = df, kernel = "linear", cost = 10, scale = FALSE)
plot(df[, c("X1", "X2")], col = dat$Y, pch = 19, xlim = c(0, 5), ylim = c(0, 5),
xlab = "X1", ylab = "X2", main = "Manual Hyperplane (not optimal)")
svmfit <- svm(Y ~ ., data = df, kernel = "linear", cost = 10, scale = FALSE)
plot(df[, c("X1", "X2")], col = df$Y, pch = 19, xlim = c(0, 5), ylim = c(0, 5),
xlab = "X1", ylab = "X2", main = "Manual Hyperplane (not optimal)")
points(svmfit$SV, pch = 5, cex = 2)
w <- t(svmfit$coefs) %*% svmfit$SV
b <- -svmfit$rho
abline(a = -b / w[2], b = -w[1] / w[2], col = "black", lwd = 2)
abline(a = 6.67, b = -2/3, col = "red", lty = 2, lwd = 2)
legend("topright", legend = c("Optimal Hyperplane", "Non-optimal Hyperplane"),
col = c("black", "red"), lty = c(1, 2), lwd = 2)
legend("bottomright", legend = c("Optimal Hyperplane", "Non-optimal Hyperplane"),
col = c("black", "red"), lty = c(1, 2), lwd = 2)
rm(list = ls(all=TRUE))
P<-c("readr","tidyr","ggplot2", "e1071", "LiblineaR", "ISLR2")
for (i in 1:length(P)) {
ifelse(!require(P[i],character.only=TRUE),install.packages(P[i]),
print(":)"))
library(P[i],character.only=TRUE)
}
rm(P,i)
# a.
df <- data.frame(
X1 = c(3, 2, 4, 1, 2, 4, 4),
X2 = c(4, 2, 4, 4, 1, 3, 1),
Y = factor(c("Red", "Red", "Red", "Red", "Blue", "Blue", "Blue"))
)
colors <- ifelse(df$Y == "Red", "red", "blue")
plot(df$X1, df$X2, col = colors, pch = 19,
xlab = "X1", ylab = "X2", main = "Scatter Plot")
legend("topright", legend = c("Red", "Blue"), col = c("red", "blue"), pch = 19)
# b.
svmfit <- svm(Y ~ X1 + X2, data = df, kernel = "linear", cost = 10, scale = FALSE)
plot(svmfit, df,
symbolPalette = c("red", "blue"))
# c.
## Extract the weight vector
w <- t(svmfit$coefs) %*% svmfit$SV
b <- -svmfit$rho
cat("β0 (intercept):", round(b, 3), "\n")
cat("β1 (X1 coefficient):", round(w[1], 3), "\n")
cat("β2 (X2 coefficient):", round(w[2], 3), "\n")
# d.
# Define the slope and intercept for the margin lines
slope <- -w[1] / w[2]
intercept <- -b / w[2]        # decision boundary
margin1 <- (1 - b) / w[2]     # upper margin
margin2 <- (-1 - b) / w[2]    # lower margin
plot(df[, c("X1", "X2")], col = df$Y, pch = 19, xlim = c(0, 5), ylim = c(0, 5))
points(svmfit$SV, pch = 5, cex = 2)
abline(a = intercept, b = slope, col = "black", lwd = 2)
abline(a = margin1, b = slope, col = "blue", lty = 2)
abline(a = margin2, b = slope, col = "blue", lty = 2)
# e.
points(svmfit$SV, pch = 8, cex = 2, lwd = 2)
# f.
svmfit$index #indices of support vectors
# slightly change the 7th obs
dat_moved <- df
dat_moved[7, "X1"] <- dat_moved[7, "X1"] + 0.1
dat_moved[7, "X2"] <- dat_moved[7, "X2"] + 0.1
# refit the SVM
svmfit_moved <- svm(Y ~ ., data = dat_moved, kernel = "linear", cost = 10, scale = FALSE)
# compare hyperplane coefficients
w_original <- t(svmfit$coefs) %*% svmfit$SV
b_original <- -svmfit$rho
w_moved <- t(svmfit_moved$coefs) %*% svmfit_moved$SV
b_moved <- -svmfit_moved$rho
cat("Original w:", round(w_original, 3), ", b:", round(b_original, 3), "\n")
cat("Moved w:  ", round(w_moved, 3), ", b:", round(b_moved, 3), "\n")
# g.
# SVM fit
svmfit <- svm(Y ~ ., data = df, kernel = "linear", cost = 10, scale = FALSE)
plot(df[, c("X1", "X2")], col = df$Y, pch = 19, xlim = c(0, 5), ylim = c(0, 5),
xlab = "X1", ylab = "X2", main = "Manual Hyperplane (not optimal)")
points(svmfit$SV, pch = 5, cex = 2)
# draw the optimal hyperplane
w <- t(svmfit$coefs) %*% svmfit$SV
b <- -svmfit$rho
abline(a = -b / w[2], b = -w[1] / w[2], col = "black", lwd = 2)
# draw a non-optimal hyperplane
abline(a = 6.67, b = -2/3, col = "red", lty = 2, lwd = 2)
legend("bottomright", legend = c("Optimal Hyperplane", "Non-optimal Hyperplane"),
col = c("black", "red"), lty = c(1, 2), lwd = 2)
# h.
df_new <- rbind(df, data.frame(X1 = 2.5, X2 = 3.5, Y = as.factor("Blue")))
svmfit_new <- svm(Y ~ ., data = df_new, kernel = "linear", cost = 10, scale = FALSE)
plot(df_new[, c("X1", "X2")], col = df_new$Y, pch = 19, xlim = c(0, 5), ylim = c(0, 5),
xlab = "X1", ylab = "X2", main = "Now Not Linearly Separable")
points(svmfit_new$SV, pch = 5, cex = 2)
points(2.5, 3.5, pch = 17, col = "blue", cex = 2)
legend("topright", legend = c("Red", "Blue", "Blue Dot (in Red zone)"),
col = c("red", "blue", "blue"), pch = c(19, 19, 17))
plot(df_new[, c("X1", "X2")], col = df_new$Y, pch = 19, xlim = c(0, 5), ylim = c(0, 5),
xlab = "X1", ylab = "X2", main = "Not Linearly Separable")
points(svmfit_new$SV, pch = 5, cex = 2)
points(2.5, 3.5, pch = 17, col = "blue", cex = 2)
legend("bottomright", legend = c("Red", "Blue", "Blue Dot (in Red zone)"),
col = c("red", "blue", "blue"), pch = c(19, 19, 17))
set.seed(14850)
# a. generate the data
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <- 1 * (x1^2 - x2^2 > 0)
df <- data.frame(x1 = x1, x2 = x2, y = as.factor(y))
# add polynomial features (non-linear transformations)
df$x1_sq <- x1^2
df$x2_sq <- x2^2
df$x1x2 <- x1 * x2
# Fit logt regression
glmfit <- glm(y ~ x1 + x2 + x1_sq + x2_sq + x1*x2, data = df, family = "binomial")
# create a grid of values
x1.grid <- seq(min(x1), max(x1), length = 100)
x2.grid <- seq(min(x2), max(x2), length = 100)
grid <- expand.grid(x1 = x1.grid, x2 = x2.grid)
grid$x1_sq <- grid$x1^2
grid$x2_sq <- grid$x2^2
grid$x1x2 <- grid$x1 * grid$x2
probs <- predict(glmfit, newdata = grid, type = "response")
# combine grid and predictions
grid$prob <- probs
# plot the data and decision boundary
ggplot(df, aes(x = x1, y = x2, color = y)) +
geom_point(alpha = 0.6) +
geom_contour(data = grid, aes(z = prob), breaks = 0.5, color = "black") +
labs(title = "Logistic Regression with Non-linear Features") +
theme_minimal()
set.seed(14850)
# a. generate the data
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <- 1 * (x1^2 - x2^2 > 0)
df <- data.frame(x1 = x1, x2 = x2, y = as.factor(y))
# add polynomial features
df$x1_sq <- x1^2
df$x2_sq <- x2^2
df$x1x2 <- x1 * x2
# Fit logt regression
glmfit <- glm(y ~ x1 + x2 + x1_sq + x2_sq + x1*x2, data = df, family = "binomial")
# plot the data and decision boundary
ggplot(df, aes(x = x1, y = x2, color = y)) +
geom_point(alpha = 0.6) +
labs(title = "Logistic Regression with Non-linear Features") +
theme_minimal()
ggplot(df, aes(x = x1, y = x2, color = y)) +
geom_point(alpha = 0.6, size = 2) +
labs(
title = "Simulated Data with Quadratic Decision Boundary",
x = "X1",
y = "X2",
color = "Class"
) +
theme_minimal()
ggplot(df, aes(x = x1, y = x2, color = y)) +
geom_point(alpha = 0.6, size = 2) +
labs(
title = "Data distribution",
x = "X1",
y = "X2",
color = "Class"
) +
theme_minimal()
ggplot(df, aes(x = x1, y = x2, color = y)) +
geom_point(alpha = 0.6, size = 2) +
labs(
title = "Data distribution by class",
x = "X1",
y = "X2",
color = "Class"
) +
theme_minimal()
glm_fit <- glm(y ~ x1 + x2, data = df, family = "binomial")
summary(glm_fit)
plot(glm_fit)
P<-c("readr","tidyr","ggplot2", "e1071", "LiblineaR", "ISLR2")
for (i in 1:length(P)) {
ifelse(!require(P[i],character.only=TRUE),install.packages(P[i]),
print(":)"))
library(P[i],character.only=TRUE)
}
rm(P,i)
rm(list = ls(all=TRUE))
rm(list = ls(all=TRUE))
P<-c("readr","tidyr","ggplot2", "e1071", "LiblineaR", "ISLR2")
for (i in 1:length(P)) {
ifelse(!require(P[i],character.only=TRUE),install.packages(P[i]),
print(":)"))
library(P[i],character.only=TRUE)
}
rm(P,i)
set.seed(14850)
# a. generate the data
x1 <- runif(500) - 0.5
x2 <- runif(500) - 0.5
y <- 1 * (x1^2 - x2^2 > 0)
df <- data.frame(x1 = x1, x2 = x2, y = as.factor(y))
# b. plot
ggplot(df, aes(x = x1, y = x2, color = y)) +
geom_point(alpha = 0.6, size = 2) +
labs(
title = "Data distribution by class",
x = "X1",
y = "X2",
color = "Class"
) +
theme_minimal()
# c. fit a logit reg
glm_fit <- glm(y ~ x1 + x2, data = df, family = "binomial")
summary(glm_fit)
plot(glm_fit)
summary(glm_fit)
# predict probabilities
probs <- predict(glm_linear, type = "response")
glm_fit <- glm(y ~ x1 + x2, data = df, family = "binomial")
summary(glm_fit)
# d. ####
# predict probabilities
probs <- predict(glm_fit, type = "response")
# assign predicted class: 1 if prob > 0.5, else 0
y_pred <- ifelse(probs > 0.5, 1, 0)
# add predicted class to the df
df$y_pred <- as.factor(y_pred)
# plot
ggplot(df, aes(x = x1, y = x2, color = y_pred)) +
geom_point(alpha = 0.6, size = 2) +
labs(
title = "Logistic Regression Predictions (Linear Boundary)",
x = "X1", y = "X2",
color = "Predicted Class"
) +
theme_minimal()
library(marginaleffects)
mod <- lm(mpg ~ hp * wt * am, data = mtcars)
# marginal effects
mfx <- marginaleffects(mod)
library(marginaleffects)
mod <- lm(mpg ~ hp * wt * am, data = mtcars)
# marginal effects
mfx <- marginaleffects(mod)
setwd("~/Dropbox/26SP/SODA 501/w2")
library(renv)       # Dependency management (renv.lock)
library(logger)     # Logging pipeline steps
library(tidyverse)  # Data manipulation + plotting
library(broom)      # Tidy regression outputs (for tables)
install.packages("renv", dependencies = FALSE)
library(renv)       # Dependency management (renv.lock)
library(logger)     # Logging pipeline steps
library(tidyverse)  # Data manipulation + plotting
library(broom)      # Tidy regression outputs (for tables)
renv::init()
setwd("~/Dropbox/26SP/SODA 501/w2/demo")
renv::init()
dir.create("data/raw", recursive = TRUE, showWarnings = FALSE)
dir.create("data/processed", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs/figures", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs/tables", recursive = TRUE, showWarnings = FALSE)
logger::log_threshold(DEBUG)
logger::log_appender(appender_file("analysis_log.txt"))
set.seed(123)  # Reproducible randomness for the full pipeline
log_info("Starting analysis pipeline")
library(renv)       # Dependency management (renv.lock)
library(logger)     # Logging pipeline steps
library(tidyverse)  # Data manipulation + plotting
library(broom)      # Tidy regression outputs (for tables)
dir.create("data/raw", recursive = TRUE, showWarnings = FALSE)
dir.create("data/processed", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs/figures", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs/tables", recursive = TRUE, showWarnings = FALSE)
set.seed(123)  # Reproducible randomness for the full pipeline
log_info("Starting analysis pipeline")
log_info("Loading education/income dataset from data/raw/education_income.csv")
log_info(paste("Rows loaded:", nrow(education_income_raw)))
log_info(paste("Rows loaded:", nrow(education_income_raw)))
dir.create("data/raw", recursive = TRUE, showWarnings = FALSE)
logger::log_threshold(DEBUG)
logger::log_appender(appender_file("analysis_log.txt"))
log_info("Starting analysis pipeline")
log_info("Loading education/income dataset from data/raw/education_income.csv")
log_info("Loading education/income dataset from data/raw/education_income.csv")
log_info(paste("Rows loaded:", nrow(education_income_raw)))
library(readr)
education_income <- read_csv("data/raw/education_income.csv")
View(education_income)
log_info(paste("Rows loaded:", nrow(education_income_raw)))
renv::snapshot()
logger::log_appender(appender_file("analysis_log.txt"))
set.seed(123)  # Reproducible randomness for the full pipeline
log_info("Starting analysis pipeline")
set.seed(123)  # Reproducible randomness for the full pipeline
log_info("Starting analysis pipeline")
# Expected location for this assignment:
# - data/raw/education_income.csv
log_info("Loading education/income dataset from data/raw/education_income.csv")
log_info(paste("Rows loaded:", nrow(education_income_raw)))
ls()
exists("education_income_raw")
education_income <- read_csv("data/raw/education_income.csv")
set.seed(123)  # Reproducible randomness for the full pipeline
log_info("Starting analysis pipeline")
log_info("Loading education/income dataset from data/raw/education_income.csv")
log_info(paste("Rows loaded:", nrow(education_income_raw)))
education_income_raw <- read_csv("data/raw/education_income.csv")
log_info("Loading education/income dataset from data/raw/education_income.csv")
log_info(paste("Rows loaded:", nrow(education_income_raw)))
log_info(paste("Columns loaded:", ncol(education_income_raw)))
log_info("Saving raw data copy (unchanged)")
readr::write_csv(education_income_raw, "data/raw/education_income.csv")
log_info("Cleaning education/income data")
education_income_clean <- education_income_raw |>
dplyr::mutate(
education = as.numeric(education),
income    = as.numeric(income)
) |>
dplyr::filter(!is.na(education), !is.na(income))
log_info(paste("Rows after cleaning:", nrow(education_income_clean)))
education_income_clean <- education_income_clean |>
dplyr::mutate(log_income = log(income))
education_income_log <- education_income_clean |>
dplyr::filter(is.finite(log_income))
log_info(paste("Rows with finite log(income):", nrow(education_income_log)))
log_info("Saving processed data")
readr::write_csv(education_income_clean, "data/processed/cleaned_education_income.csv")
log_info("Fitting Model 1: income ~ education")
rm(list = ls())
library(readr)
library(renv)       # Dependency management (renv.lock)
library(logger)     # Logging pipeline steps
library(tidyverse)  # Data manipulation + plotting
library(broom)      # Tidy regression outputs (for tables)
setwd("~/Dropbox/26SP/SODA 501/w2/hw")
PWD
pwd
print(wd)
renv::init() # create a project-local library and an renv.lock file.
renv::init() # create a project-local library and an renv.lock file.
renv::init()
dir.create("data/raw", recursive = TRUE, showWarnings = FALSE)
dir.create("data/processed", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs/figures", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs/tables", recursive = TRUE, showWarnings = FALSE)
getwd()
setwd("~/Dropbox/26SP/SODA 501/soda501_sp26/02_reproducibility")
rm(list = ls())
library(readr)
library(renv)       # Dependency management (renv.lock)
library(logger)     # Logging pipeline steps
library(tidyverse)  # Data manipulation + plotting
library(broom)      # Tidy regression outputs (for tables)
# renv::init() # create a project-local library and an renv.lock file.
dir.create("data/raw", recursive = TRUE, showWarnings = FALSE)
dir.create("data/processed", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs/figures", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs/tables", recursive = TRUE, showWarnings = FALSE)
library(renv)
library(logger)
library(tidyverse)
library(broom)
dir.create("data/raw", recursive = TRUE, showWarnings = FALSE)
dir.create("data/processed", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs/figures", recursive = TRUE, showWarnings = FALSE)
dir.create("outputs/tables", recursive = TRUE, showWarnings = FALSE)
log_info("=== START ANALYSIS ===")
log_info("Timestamp: {format(Sys.time(), '%Y-%m-%d %H:%M:%S %Z')}")
log_info("Working directory: {getwd()}")
input_path <- "data/raw/education_income.csv"
stopifnot(file.exists(input_path))
df_raw <- readr::read_csv(input_path, show_col_types = FALSE)
log_info("Rows loaded: {nrow(df_raw)}")
log_info("Columns: {paste(names(df_raw), collapse = ', ')}")
df <- df_raw |>
mutate(
education = as.numeric(education),
income    = as.numeric(income)
) |>
filter(!is.na(education), !is.na(income))
log_info("Rows after cleaning: {nrow(df)}")
readr::write_csv(df, "data/processed/cleaned_education_income.csv")
log_info("Wrote processed data: data/processed/cleaned_education_income.csv")
m1 <- lm(income ~ education, data = df)
m2 <- lm(income ~ education + I(education^2), data = df)
df_log <- df |>
mutate(log_income = log(income)) |>
filter(is.finite(log_income))
m3 <- lm(log_income ~ education, data = df_log)
writeLines(capture.output(summary(m1)), "outputs/tables/model_1_summary.txt")
writeLines(capture.output(summary(m2)), "outputs/tables/model_2_summary.txt")
writeLines(capture.output(summary(m3)), "outputs/tables/model_3_summary.txt")
coef_table <- bind_rows(
tidy(m1) |> mutate(model = "Model 1"),
tidy(m2) |> mutate(model = "Model 2"),
tidy(m3) |> mutate(model = "Model 3")
)
readr::write_csv(coef_table, "outputs/tables/regression_coefficients.csv")
p <- ggplot(df, aes(x = education, y = income)) +
geom_point() +
geom_smooth(method = "lm", se = TRUE) +
labs(
title = "Income vs Education",
subtitle = "Linear fit (Model 1)",
x = "Education",
y = "Income"
)
ggsave(
"outputs/figures/education_income_scatter.png",
plot = p,
width = 7,
height = 5,
dpi = 300
)
writeLines(capture.output(sessionInfo()), "outputs/session_info.txt")
renv::snapshot()
